<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Real-Time Radiance Fields for Single-Image Portrait View Synthesis</title>
      <link href="/2023/10/10/real-time-radiance-fields-for-single-image-portrait-view-synthesis/"/>
      <url>/2023/10/10/real-time-radiance-fields-for-single-image-portrait-view-synthesis/</url>
      
        <content type="html"><![CDATA[<h4 id="Real-Time-Radiance-Fields-for-Single-Image-Portrait-View-Synthesis"><a href="#Real-Time-Radiance-Fields-for-Single-Image-Portrait-View-Synthesis" class="headerlink" title="Real-Time Radiance Fields for Single-Image Portrait View Synthesis"></a>Real-Time Radiance Fields for Single-Image Portrait View Synthesis</h4><h5 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h5><p>大问题：Digitally reproducing the 3D appearance of an object from a single<br>image</p><p>具体问题：infer a 3D representation for real-time view synthesis given a single<br>portrait-style input image</p><p>方法：we train an encoder end-to-end that directly predicts the triplane 3D<br>features from a single input image.</p><p>we fully supervise the training of our triplane encoder<br>for novel view synthesis using multiview-consistent synthetic data<br>generated from a pre-trained 3D GAN. Together with our data aug-<br>mentation strategies and Transformer-based encoder</p><h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><h6 id="Triplane-encoder"><a href="#Triplane-encoder" class="headerlink" title="Triplane encoder"></a>Triplane encoder</h6><p>实现目标：<img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20231010184806154.png" alt="image-20231010184806154"></p><p>开发并训练一个混合卷积编码器，实现从未设置的RGB图像I映射到规范的三平面表示</p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20231010205450148.png" alt="image-20231010205450148"></p><p>编码器结构：DeepLabV3提取图像的robust低分辨率特征，𝑭low =DeepLabV3(𝑰)，</p><p>之后𝑭low输入到Vision Transformer and CNN中，为中间输出特征提供全局归纳偏差。𝑭 = Conv(ViT(𝑭low)),此时特征F已经成功创建了目标的规范化 3D 表示（并且在我们的实验过程中发现，这个浅层编码器足以合理地规范化主题，但无法表示重要的高频或主题-具体细节，例如头发或胎记。）</p><p>仅使用编码器 Ehigh 的单个下采样阶段再次对图像进行卷积编码，以获得特征 𝑭high = Ehigh(𝑰)</p><p>𝑭high与提取的全局特征𝑭连接起来，并通过另一个 Vision Transformer，最终被解码为带有卷积的三平面，</p><p>编码器的输出具有以下形式： 𝑻 = E(𝑰) = Conv(ViT(𝑭 ⊕ 𝑭high)),  其中 ⊕ 表示沿通道轴的串联，𝑻 是 三平面特征表示。</p><h6 id="Training-with-synthetic-data"><a href="#Training-with-synthetic-data" class="headerlink" title="Training with synthetic data"></a>Training with synthetic data</h6><p>经过训练后，3D GAN 生成器可以被冻结并通过 GAN 反转用于单图像 3D 重建任务</p><p>使用合成数据进行训练三平面编码器，合成数据由EG3D生成。（对潜在向量进行采样并将其传递给 EG3D 生成器会产生相应的三平面 𝑻），通过给定相机参数P（a focal length, principal point, camera orientation and position），我们可以渲染来自冻结的 EG3D 生成器和𝑻的任何图像。</p><p>每个梯度步骤中，我们从参考（输入）相机𝑷ref 和另一个相机𝑷mv 合成两个具有相同身份（相同潜在代码）的图像，以进行多视图监督，每个渲染通道将为我们提供四个图像：𝑰𝑓、𝑰128、𝑰512 和𝑰𝐷，</p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20231010212353562.png" alt="image-20231010212353562"></p><p>𝑻 = E(𝑰512)，使用 𝑻 来调节来自相机 𝑷ref 和 𝑷mv 的体渲染过程，以获得另外两组四张图像，我们将其表示为 𝑰𝑓、𝑰128、𝑰512 和 𝑰𝐷 </p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20231010212911903.png" alt="image-20231010212911903"></p><p>损失L直观地比较了由 EG3D 合成的数量和由我们的编码器创建的数量，以及生成对抗目标</p><p>𝐿tri 是 𝑻 和之间的 L1 损失</p><p>𝐿col 是两组对 (𝑰128, 𝑰128) 和 (𝑰512, 𝑰512) 之间计算的平均 L1 损失</p><p>𝐿LPIPS 是 LPIPS 感知损失 [Zhang et al. 2018] 对两组对 (𝑰128, 𝑰128) 和 (𝑰512, 𝑰512) 进行计算</p><p>𝐿feat 是对 (𝑰𝑓, 𝑰𝑓) 之间计算的平均 L1 损失</p><p>𝐿adv 是使用来自 EG3D 的预训练双判别器的对抗性损失，该判别器在训练过程中进行了微调</p><p>𝐿cate 是可选的特定类别损失</p><h5 id="On-the-fly-augmentation"><a href="#On-the-fly-augmentation" class="headerlink" title="On-the-fly augmentation."></a>On-the-fly augmentation.</h5><p>之前采用合成数据进行训练可能使得模型缺乏推广到真实数据的能力，因此我们增强了标准 EG3D 渲染方法，该方法假定固定的相机胶卷、焦距、主点和距拍摄对象的距离。从随机分布中对所有四个值进行采样来选择相机参数𝑷ref。 𝑷mv的相机参数，我们选择 EG3D 模型中的固定值。</p><p>对于𝑷ref，我们从相对于人脸正面的±26°的俯仰范围和±49°的偏航范围对相机进行采样</p><p>对于 𝑷mv，我们从相对于人脸正面的 ±26° 俯仰范围和 ±36° 偏航范围对相机进行采样</p><p>这使得我们的模型的监督能够在高度可变的相机姿势下进行，迫使模型学习有效地规范化并从具有挑战性的图像中进行推断</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>pytorch学习笔记</title>
      <link href="/2023/10/10/pytorch-xue-xi-bi-ji/"/>
      <url>/2023/10/10/pytorch-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>pytorch学习笔记</p><p>一，数据加载</p><p>pytorch数据集加载包含以下两个部分：</p><p>Dataset：提供一种方式去获取数据及其label</p><p>Dataloader:为网络提供不同的数据形式</p><p>需要实现的功能：如何获取每一个数据及其label，总共有多少数据？</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">"D:\\test program\\dataset\\train\\ants_image\\0013035.jpg"</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"><span class="comment"># print(img.size)</span></span><br><span class="line"><span class="comment"># img.show()</span></span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">"dataset/train"</span></span><br><span class="line">label_dir = <span class="string">"ants_image"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)</span><br><span class="line">        self.img_path = os.listdir(self.path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_name = self.img_path[idx]</span><br><span class="line">        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)</span><br><span class="line">        label = self.label_dir</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ants_dataset = MyData(root_dir, label_dir)</span><br><span class="line">img, label = ants_dataset[<span class="number">1</span>]</span><br><span class="line">img.show()</span><br><span class="line"><span class="comment">#获取数据集中总共有多少数据</span></span><br><span class="line"><span class="built_in">len</span> = <span class="built_in">len</span>(ants_dataset)</span><br><span class="line"><span class="comment">#获取每一个数据及其label</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>):</span><br><span class="line">    img, label=ants_dataset[i]</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基于pytorch的猫鼠大战</title>
      <link href="/2023/10/10/ji-yu-pytorch-de-mao-shu-da-zhan/"/>
      <url>/2023/10/10/ji-yu-pytorch-de-mao-shu-da-zhan/</url>
      
        <content type="html"><![CDATA[<h4 id="基于pytorch的猫鼠大战"><a href="#基于pytorch的猫鼠大战" class="headerlink" title="基于pytorch的猫鼠大战"></a>基于pytorch的猫鼠大战</h4><h5 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h5><p>这段时间学习了pytorch框架进行了简单入门，于是便想着做一个简单实战项目，总结一下所学知识。</p><p>之前从未学习过深度学习相关知识，在入门的过程中总是感觉云里雾里，不知道怎么办。</p><p>就一个项目而言，理清我们要做的大致框架，做起来会清晰很多。深度学习项目的大致框架一般都是，数据集预处理，加载数据集，构建神经网络，设置损失函数和优化器，定义训练函数开始训练。</p><p>下面我将从上面这个大致框架开始实现猫鼠大战。</p><h5 id="二、猫鼠大战数据集"><a href="#二、猫鼠大战数据集" class="headerlink" title="二、猫鼠大战数据集"></a>二、猫鼠大战数据集</h5><p>猫鼠大战数据集采用的是kaggle竞赛网站的猫鼠大战竞赛的数据集<a href="https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset">Cats-vs-Dogs | Kaggle</a></p><h5 id="三、实现流程"><a href="#三、实现流程" class="headerlink" title="三、实现流程"></a>三、实现流程</h5><h6 id="首先导入我们所需要的库"><a href="#首先导入我们所需要的库" class="headerlink" title="首先导入我们所需要的库"></a>首先导入我们所需要的库</h6><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 划分训练集和验证集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></tbody></table></figure><h6 id="设置图片预处理"><a href="#设置图片预处理" class="headerlink" title="设置图片预处理"></a>设置图片预处理</h6><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 图片预处理</span></span><br><span class="line">transforms = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整图像大小为 (224, 224)</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.255</span>])</span><br><span class="line">])</span><br></pre></td></tr></tbody></table></figure><h6 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h6><p>pytorch的加载数据集需要使用datasets获取数据和DataLoader加载数据</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 设置训练数据集的路径，获取数据集数据并预处理，从训练数据集中划分验证集，设置批处理大小，分批加载数据</span></span><br><span class="line">train_data_dir = <span class="string">"data/catsdogs/catsdogs/train"</span></span><br><span class="line">train_data = datasets.ImageFolder(train_data_dir, transforms)</span><br><span class="line">train_data, val_data = train_test_split(train_data, test_size=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">1</span>)</span><br><span class="line">val_loader = data.DataLoader(val_data, batch_size=batch_size)</span><br><span class="line"><span class="comment"># 这里加载测试集</span></span><br><span class="line">test_data_dir = <span class="string">"data/catsdogs/catsdogs/test"</span></span><br><span class="line">test_data = datasets.ImageFolder(test_data_dir, transforms)</span><br><span class="line">test_loader = data.DataLoader(test_data, batch_size=batch_size)</span><br></pre></td></tr></tbody></table></figure><h6 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h6><p>这里我直接使用了预训练好的resnet网络，大家也可以自行搭建或使用其它网络</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">resnet50 = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">num_ftrs = resnet50.fc.in_features</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> resnet50.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span>  <span class="comment"># False：冻结模型的参数，也就是采用该模型已经训练好的原始参数。只需要训练我们自己定义的Linear层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保持in_features不变，修改out_features=2，因为只有猫鼠两类</span></span><br><span class="line">resnet50.fc = nn.Sequential(nn.Linear(num_ftrs, <span class="number">2</span>),</span><br><span class="line">                            nn.LogSoftmax(dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model = resnet50.to(device)</span><br></pre></td></tr></tbody></table></figure><p>例如我们可以构建一个简单的神经网络</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1=nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">5</span>)</span><br><span class="line">        self.conv2=nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x=F.max_pool2d(F.relu(self.conv1(x)),<span class="number">2</span>)</span><br><span class="line">        x=F.max_pool2d(F.relu(self.conv2(x)),<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">model=Net()</span><br></pre></td></tr></tbody></table></figure><h6 id="设置优化器"><a href="#设置优化器" class="headerlink" title="设置优化器"></a>设置优化器</h6><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 使用学习率调度器</span></span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> StepLR</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">scheduler = StepLR(optimizer, step_size=<span class="number">10</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># 这里使用了Adam优化器，并使用学习率调度器不断自主更新学习率，提升训练效果</span></span><br></pre></td></tr></tbody></table></figure><p>如果想记录模型在训练过程中的变换，可以使用tensorboard</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">'''设置TensorBoard'''</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">"logs"</span>)</span><br></pre></td></tr></tbody></table></figure><h6 id="定义训练函数"><a href="#定义训练函数" class="headerlink" title="定义训练函数"></a>定义训练函数</h6><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, device, train_loader, optimizer, epoch, losses</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> idx, (t_data, t_target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        t_data, t_target = t_data.to(device), t_target.to(device)</span><br><span class="line">        pred = model(t_data)</span><br><span class="line">        loss = F.nll_loss(pred, t_target)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> idx % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"epoch:{}, iteration:{}, loss:{}"</span>.<span class="built_in">format</span>(epoch, idx, loss.item()))</span><br><span class="line">            <span class="comment"># 将batch训练的平均loss 写入到tensorboard</span></span><br><span class="line">            avg_loss = total_loss / (idx + <span class="number">1</span>)</span><br><span class="line">            writer.add_scalar(<span class="string">'train_batch_loss'</span>, avg_loss, epoch * <span class="built_in">len</span>(train_loader) + idx)</span><br><span class="line"></span><br><span class="line">    avg_epoch_loss = total_loss / <span class="built_in">len</span>(train_loader)</span><br><span class="line">    losses.append(avg_epoch_loss)</span><br><span class="line">    <span class="comment"># 将epoch的平均loss写入tensorboard</span></span><br><span class="line">    writer.add_scalar(<span class="string">'train_epoch_loss'</span>, avg_epoch_loss, epoch)</span><br><span class="line">    <span class="comment"># writer.add_scalar('train_epoch_accuracy', val_acc, epoch)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validate</span>(<span class="params">model, device, val_loader</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total_samples = <span class="built_in">len</span>(val_loader.dataset)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> idx, (v_data, v_target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(val_loader):</span><br><span class="line">            v_data, v_target = v_data.to(device), v_target.to(device)</span><br><span class="line">            pred = model(v_data)</span><br><span class="line">            pred_class = pred.argmax(dim=<span class="number">1</span>)</span><br><span class="line">            correct += pred_class.eq(v_target.view_as(pred_class)).<span class="built_in">sum</span>().item()</span><br><span class="line">    acc = correct / total_samples</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model, device, test_loader</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> idx, (t_data, t_target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):</span><br><span class="line">            t_data, t_target = t_data.to(device), t_target.to(device)</span><br><span class="line">            pred = model(t_data)</span><br><span class="line">            pred_class = pred.argmax(dim=<span class="number">1</span>)</span><br><span class="line">            correct += pred_class.eq(t_target.view_as(pred_class)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    acc = correct / <span class="built_in">len</span>(test_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"accuracy: {}"</span>.<span class="built_in">format</span>(acc))</span><br><span class="line">    <span class="comment"># 将测试准确率写入tensorboard</span></span><br><span class="line">    writer.add_scalar(<span class="string">'test_accuracy'</span>, acc, epoch)</span><br></pre></td></tr></tbody></table></figure><h6 id="开始训练并保存训练结果"><a href="#开始训练并保存训练结果" class="headerlink" title="开始训练并保存训练结果"></a>开始训练并保存训练结果</h6><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    num_epochs = <span class="number">30</span>  <span class="comment"># 增加训练轮次</span></span><br><span class="line">    losses = []</span><br><span class="line">    best_val_acc = <span class="number">0.0</span>  <span class="comment"># 记录最佳验证集准确率</span></span><br><span class="line">    <span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">    begin_time = time()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train(model, device, train_loader, optimizer, epoch, losses)</span><br><span class="line">        val_acc = validate(model, device, val_loader)</span><br><span class="line">        test(model, device, test_loader)  <span class="comment"># 在每个epoch结束后进行一次测试</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Validation accuracy after epoch {}: {}"</span>.<span class="built_in">format</span>(epoch, val_acc))</span><br><span class="line"></span><br><span class="line">        scheduler.step()  <span class="comment"># 更新学习率</span></span><br><span class="line">        avg_val_acc = validate(model, device, val_loader)</span><br><span class="line">        writer.add_scalar(<span class="string">'val_epoch_accuracy'</span>, avg_val_acc, epoch)</span><br><span class="line">        <span class="comment"># 保存在验证集上性能最佳的模型参数</span></span><br><span class="line">        <span class="keyword">if</span> val_acc &gt; best_val_acc:</span><br><span class="line">            best_val_acc = val_acc</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">"best_model.pt"</span>)</span><br><span class="line"></span><br><span class="line">    end_time = time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载并测试最佳模型</span></span><br><span class="line">    best_model = resnet50.to(device)</span><br><span class="line">    best_model.load_state_dict(torch.load(<span class="string">"best_model.pt"</span>))</span><br><span class="line">    test(best_model, device, test_loader)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存完整模型</span></span><br><span class="line">    torch.save(best_model, <span class="string">"Resnet50_final.pt"</span>)</span><br><span class="line">    <span class="comment"># 关闭tensorboard的writer</span></span><br><span class="line">    writer.close()</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>张量</title>
      <link href="/2023/10/10/zhang-liang/"/>
      <url>/2023/10/10/zhang-liang/</url>
      
        <content type="html"><![CDATA[<p>张量是一种特殊的数据结构，是MindSpore网络运算中的基本数据结构。</p><p>创建张量</p><p>张量的创建方式有多种，构造张量时，支持传入<code>Tensor</code>、<code>float</code>、<code>int</code>、<code>bool</code>、<code>tuple</code>、<code>list</code>和<code>numpy.ndarray</code>类型。</p><ul><li><p><strong>根据数据直接生成</strong></p><p>可以根据数据创建张量，数据类型可以设置或者通过框架自动推断。</p></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">data = [1, 0, 1, 0]</span><br><span class="line">x_data = Tensor(data)</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>从NumPy数组生成</strong></p><p>可以从NumPy数组创建张量。</p></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = Tensor(np_array)</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>使用init初始化器构造张量</strong></p><p>当使用<code>init</code>初始化器对张量进行初始化时，支持传入的参数有<code>init</code>、<code>shape</code>、<code>dtype</code>。</p><ul><li><code>init</code>: 支持传入<a href="https://www.mindspore.cn/docs/zh-CN/r2.0/api_python/mindspore.common.initializer.html">initializer</a>的子类。</li><li><code>shape</code>: 支持传入 <code>list</code>、<code>tuple</code>、 <code>int</code>。</li><li><code>dtype</code>: 支持传入<a href="https://www.mindspore.cn/docs/zh-CN/r2.0/api_python/mindspore/mindspore.dtype.html#mindspore.dtype">mindspore.dtype</a>。</li></ul></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">from mindspore.common.initializer import One, Normal</span><br><span class="line"></span><br><span class="line"># Initialize a tensor with ones</span><br><span class="line">tensor1 = mindspore.Tensor(shape=(2, 2), dtype=mindspore.float32, init=One())</span><br><span class="line"># Initialize a tensor from normal distribution</span><br><span class="line">tensor2 = mindspore.Tensor(shape=(2, 2), dtype=mindspore.float32, init=Normal())</span><br><span class="line"></span><br><span class="line">print("tensor1:\n", tensor1)</span><br><span class="line">print("tensor2:\n", tensor2)</span><br><span class="line">tensor1:</span><br><span class="line"> [[1. 1.]</span><br><span class="line"> [1. 1.]]</span><br><span class="line">tensor2:</span><br><span class="line"> [[-0.00063482 -0.00916224]</span><br><span class="line"> [ 0.01324238 -0.0171206 ]]</span><br><span class="line">`init`主要用于并行模式下的延后初始化，在正常情况下不建议使用init对参数进行初始化。</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>继承另一个张量的属性，形成新的张量</strong></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">from mindspore import ops</span><br><span class="line"></span><br><span class="line">x_ones = ops.ones_like(x_data)</span><br><span class="line">print(f"Ones Tensor: \n {x_ones} \n")</span><br><span class="line"></span><br><span class="line">x_zeros = ops.zeros_like(x_data)</span><br><span class="line">print(f"Zeros Tensor: \n {x_zeros} \n")</span><br><span class="line">Ones Tensor:</span><br><span class="line"> [1 1 1 1]</span><br><span class="line"></span><br><span class="line">Zeros Tensor:</span><br><span class="line"> [0 0 0 0]</span><br></pre></td></tr></tbody></table></figure><h2 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h2><p>张量的属性包括形状、数据类型、转置张量、单个元素大小、占用字节数量、维数、元素个数和每一维步长。</p><ul><li>形状（shape）：<code>Tensor</code>的shape，是一个tuple。</li><li>数据类型（dtype）：<code>Tensor</code>的dtype，是MindSpore的一个数据类型。</li><li>单个元素大小（itemsize）： <code>Tensor</code>中每一个元素占用字节数，是一个整数。</li><li>占用字节数量（nbytes）： <code>Tensor</code>占用的总字节数，是一个整数。</li><li>维数（ndim）： <code>Tensor</code>的秩，也就是len(tensor.shape)，是一个整数。</li><li>元素个数（size）： <code>Tensor</code>中所有元素的个数，是一个整数。</li><li>每一维步长（strides）： <code>Tensor</code>每一维所需要的字节数，是一个tuple。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>复试</title>
      <link href="/2023/01/10/fu-shi/"/>
      <url>/2023/01/10/fu-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="操作系统复试集合："><a href="#操作系统复试集合：" class="headerlink" title="操作系统复试集合："></a>操作系统复试集合：</h2><p>操作系统的基本功能</p><p>操作系统的基本功能是指控制和管理计算机系统的硬件和软件资源，并提供给用户和其他软件方便的接口和环境⁴。操作系统的基本功能可以分为以下几类¹²³：</p><ul><li>进程管理：负责进程的创建、调度、同步、通信和终止。</li><li>存储管理：负责存储器的分配、共享、保护和扩展。</li><li>设备管理：负责设备的分配、传输控制和独立性。</li><li>文件管理：负责文件的创建、删除、读写、保护和目录结构。</li><li>作业管理：负责作业的输入输出控制、优先级调整和资源分配。</li></ul><p>死锁及其产生的原因</p><p>所谓死锁，是指多个进程因竞争资源而造成的一种互相等待的僵局，若无外力作用，这些进程将无法向前推进。</p><p>产生原因：</p><p>1.对系统中不可剥夺资源的竞争，不可剥夺资源的数量难以满足多个进程的需要</p><p>2.进程推荐顺序非法：进程在运行过程中，请求和释放资源的顺序不当；信号量的使用不当（进程彼此等待对方发来的消息）</p><p>死锁产生的必要条件：互斥条件，不剥夺条件，请求并保持条件，循环等待条件</p><p>进程和线程的区别</p><p>进程和线程的区别主要有以下几点¹²³：</p><ul><li>进程是资源分配的基本单位，线程是处理机调度的基本单位。</li><li>进程拥有独立的内存空间，线程共享进程的内存空间。</li><li>进程之间的切换开销大，线程之间的切换开销小。</li><li>进程之间的通信需要借助操作系统，线程之间的通信可以直接进行。</li></ul><p>一个线程只能属于一个进程，一个进程拥有多个线程。、</p><p>对进程管理的理解</p><p>进程管理是操作系统的一个重要功能，它涉及到进程的创建、调度、同步、通信和终止等方面1。进程管理的目的是为了有效地利用系统资源，提高系统的吞吐量和响应速度，保证进程的安全性和可靠性2。</p><p>银行家算法</p><p>进程运行之前先声明对各种资源的最大需求量，当进程在执行中继续申请资源时，先测试该进程已占用的资源数与本次申请的资源数之和是否超过该进程申明的最大需求量，超过则拒绝分配资源。未超过则测试系统现存的资源能否满足该进程尚需的最大资源量。若能满足则尝试分配给该进程，之后执行安全性算法，若分配后系统处于安全状态才正式分配资源给该进程。</p><p>文件系统的三个模式</p><p>对象及其属性：文件管理系统管理的对象有：① 文件。它作为文件管理的直接对象。② 目录。为了方便用户对文件的存取和检索，在文件系统中必须配置目录，每个目录项中，必须含有文件名及该文件所在的物理地址(或指针)。对目录的组织和管理是方便用户和提高对文件存取速度的关键。③ 磁盘(磁带)存储空间。文件和目录必定占用存储空间，对这部分空间的有效管理，不仅能提高外存的利用率，而且能提高对文件的存取速度。</p><p>对对象操纵和管理的软件集合：这是文件管理系统的核心部分。文件系统的功能大多是在这一层实现的，其中包括: 对文件存储空间的管理、对文件目录的管理、用于将文件的逻辑地址转换为物理地址的机制、对文件读和写的管理，以及对文件的共享与保护等功能。</p><p>文件系统的接口：</p><ul><li>命令接口。 这是指作为用户与文件系统交互的接口。 用户可通过键盘终端键入命令，取得文件系统的服务。</li><li>程序接口。这是指作为用户程序与文件系统的接口。用户程序可通过系统调用来取得文件系统的服务。</li></ul><h2 id="数据库复试："><a href="#数据库复试：" class="headerlink" title="数据库复试："></a>数据库复试：</h2><p>事务的特性ACID</p><p>事务的特性是指事务应该具有的四个属性，分别是原子性、一致性、隔离性和持久性¹²。原子性是指事务是一个不可分割的工作单位，要么全部成功，要么全部失败²。一致性是指事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态⁵。隔离性是指多个事务并发执行时，每个事务都不受其他事务的影响²。持久性是指事务一旦提交，它对数据库的修改就是永久的，即使发生故障也不会丢失²。</p><p>er图的作用</p><p>ER图是实体-联系图的简称，是一种用来描述现实世界的概念模型的方法¹²。它可以表示实体类型、属性和联系，方便设计数据库²³。</p><p>基本表和视图的区别与联系</p><p>基本表是真实存在于数据库中的表，存储了真实的数据¹²。视图是由一个或多个基本表导出的虚拟的表，不存储真实的数据，只存储数据的定义¹²³。视图可以方便用户查询和操作数据，而不需要直接接触基本表²⁴。</p><p>数据库管理系统的主要功能</p><p>数据库管理系统是一种操纵和管理数据库的大型软件，用于建立、使用和维护数据库¹³。它的主要功能有：数据定义、数据操作、数据组织、存储与管理、数据库的运行管理、数据库的保护和数据库的维护²⁶。</p><p>数据库的三级模式和两级映像</p><p>三级模式指的是外模式，模式，内模式</p><p>模式也称逻辑模式，是数据库中全体数据的逻辑结构和特征的描述，是实际数据库中的全部数据逻辑存储</p><p>外模式也称用户模式，是数据库用户能够看见和使用的局部数据的逻辑结构和特征的描述，是数据库用户使用的局部数据视图。</p><p>内模式是存储模式，是数据库中数据的物理存储方式</p><p>为了实现三级模式之间的联系和转换，数据库提出了二级映像即外模式/模式映像，模式/内模式映像</p><p>外模式/模式映像保障了数据的逻辑独立性，模式改变时，外模式不必改变</p><p>模式/内模式映像保障了数据的物理独立性，内模式改变时，模式不必改变</p><p>什么是封锁协议，不同级别的封锁协议有什么区别</p><p>封锁协议是指在运用X锁和S锁对数据对象加锁时需要遵照的一些规则¹。不同级别的封锁协议有以下区别：</p><ul><li>一级封锁协议：事务在修改数据前，必须先对数据加X锁，直到事务结束才释放²³。可以防止丢失修改，但不能避免读脏数据³。</li><li>二级封锁协议：在一级的基础上，事务在读取数据前，必须先对数据加S锁，读完后立即释放²⁴。可以防止丢失修改和读脏数据，但不能保证可重复读⁴。</li><li>三级封锁协议：在一级的基础上，事务在读取数据前，必须先对数据加S锁，直到事务结束才释放²⁵。可以防止丢失修改，读脏数据和不可重复读⁵。</li></ul><p>三个模型</p><p>层次模型</p><p>层次模型是数据库系统最早使用的一种模型，它的数据结构是一棵“有向树”。根结点在最上端，层次最高，子结点在下，逐层排列。层次模型的特征是：</p><p>有且只有一个根结点<br>其他结点有且仅有一个父结点</p><p>网状模型</p><p>网状模型以网状结构表示实体与实体之间的联系。网中的每一个结点代表一个记录类型，联系用链接指针来实现。网状模型可以表示多个从属关系的联系，也可以表示数据间的交叉关系，即数据间的横向关系与纵向关系，它是层次模型的扩展。网状模型可以方便地表示各种类型的联系，但结构复杂，实现的算法难以规范化。其特征是：</p><p>允许结点有多于一个父结点；<br>可以有一个以上的结点没有父结点。</p><p>关系模型</p><p>关系模型以二维表结构来表示实体与实体之间的联系，它是以关系数学理论为基础的。关系模型的数据结构是一个“二维表框架”组成的集合。每个二维表又可称为关系。在关系模型中，操作的对象和结果都是二维表。关系模型是目前最流行的数据库模型。支持关系模型的数据库管理系统称为关系数据库管理系统，Access就是一种关系数据库管理系统。</p><p>描述的一致性，不仅用关系描述实体本身，而且也用关系描述实体之间的联系；<br>可直接表示多对多的联系；<br>关系必须是规范化的关系，即每个属性是不可分的数据项，不许表中有表；<br>关系模型是建立在数学概念基础上的，有较强的理论依据。</p><p>drop delete where having</p><p>drop语句将删除表的结构以及它依赖的约束</p><p>delete 删除表的记录</p><p> where用于查询满足指定条件的元组</p><p>having和group by子句配套使用，用于对分组后的数据按照一定的条件进行筛选，最终只输出满足条件的组，指定条件一般是聚合函数，having作用与组，where作用与元组即基本表</p><p>范式</p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20230311193536211.png" alt="image-20230311193536211"></p><p>所谓第一范式，就是数据表的列不可再分。<br>第二范式是指，首先满足第一范式，并且表中非主属性不存在对主属性的部分依赖。<br>第三范式定义是，满足第二范式，并且消除了非主属性对于码的传递函数依赖。</p><p>BC范式在 3NF 的基础上消除主属性对于码的部分与传递函数依赖。（主属性【仓库名】对于码【（管理员，物品名）】的部分函数依赖。</p><h2 id="数据结构复试："><a href="#数据结构复试：" class="headerlink" title="数据结构复试："></a>数据结构复试：</h2><p>查找算法</p><p>静态查找（无需动态的修改查找表）：顺序查找，折半查找（适用于有序的顺序表），分块查找（分为若干个子块，块内可以无序，块间有序）索引表中含有各块最大关键字和各块中的第一个元素的地址，索引表按关键字有序排列。先查索引表确定块的位置，然后块内顺序查找</p><p>动态查找：二叉排序树（从根节点开始，沿着某个分支逐层向下比较），平衡二叉树(任意节点的左右子树高度差的绝对值不超过1的二叉排序树)</p><p>B/B+树，散列表</p><p>快排思想</p><p>图的遍历</p><p>广度优先遍历</p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20230315173540290.png" alt="image-20230315173540290"></p><p>深度优先遍历</p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20230315174601436.png" alt="image-20230315174601436"></p><p>算法好坏的判定</p><p>一个算法的好坏主要从时间复杂度和空间复杂度来判断，时间复杂度是指执行算法所需要的计算工作量，他定量描述了该算法的运行时间.</p><p>空间复杂度是对一个算法在运行过程中所需要消耗的内存空间.</p><p>还有正确性，可读性，健壮性</p><p>栈和队列的区别</p><p>队列（Queue）：是限定只能在表的一端进行插入和在另一端进行删除操作的<a href="https://so.csdn.net/so/search?q=线性表&amp;spm=1001.2101.3001.7020">线性表</a>；先进先出</p><p>栈（Stack）：是限定只能在表的一端进行插入和删除操作的线性表。先进后出</p><p>最短路径算法包括</p><p>BFS：适用无权图</p><p>迪杰斯特拉算法：不适用与负权值的图，算法思想是选定一个开始顶点，构建三个数组，分别用于标记节点是否已找到最短路径，初始顶点到当前顶点的路径长度，当前顶点的前驱。首先从初始顶点开始寻找与它最近的顶点，找到后就更新路径长度数组，标记已找到最短路径的顶点。重复以上步骤，直到所有顶点均已找到最短评价路径。</p><p>弗洛伊德算法：不适用与负权值的图，求出每一对顶点之间的最短路径。使用动态规划思想，将复杂的问题分解成多个小问题。初始不允许选定的两个顶点之间存在中转节点，求出最短路径，然后允许v0作为中转节点，求出最短路径，……..，经过n轮后得到最短路径。</p><h2 id="计算机网络复试："><a href="#计算机网络复试：" class="headerlink" title="计算机网络复试："></a>计算机网络复试：</h2><p>TCP,UDP：</p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20230309202248100.png" alt="image-20230309202248100"></p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20230312102521544.png" alt="image-20230312102521544"></p><p>TCP流量控制是利用其滑动窗口机制实现的，根据接收方接受缓存的大小，调整发送窗口大小，让接收方来的及接收。</p><p>拥塞控制</p><p>发送窗口取接收窗口和拥塞窗口间的最小值</p><p>慢开始和拥塞控制：拥塞窗口报文段从1开始，按传输轮次k每次2^k增加，直到达到慢开始门限值，开启拥塞控制，每个传输轮次加1，直到检测到发生拥塞，将拥塞窗口重新置为1，慢开始门限值置为之前的一半。重复以上步骤。</p><p>快重传和快恢复：收到三个重复的确认报文后，执行快重传算法即在重传时间内快速重传丢失报文段。快恢复是指发生拥塞后，将拥塞窗口直接降到慢开始门限值，继续线性加1.</p><p>三次握手</p><p>“三次握手四次挥手” 是计算机网络中用来描述在两个设备之间建立和终止连接的过程的术语。“三方握手”是指在两个设备之间建立连接的过程，而“四方握手”则是指终止两个设备间连接的过程。</p><p>在三方握手期间，客户端向服务器发送SYN包，服务器以SYN-ACK包进行响应。然后，客户端向服务器发送ACK包，服务器完成连接建立过程。</p><p>在四向握手期间，客户端向服务器发送FIN包，服务器用ACK包进行响应。然后，服务器向客户端发送FIN包，客户端以ACK包进行响应。这就完成了连接终止过程。</p><p>五层参考模型</p><p>OSI七层参考模型是一个法定的标准，而TCP/IP模型是一个事实标准。</p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20230312115311332.png" alt="image-20230312115311332"></p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20230312115542257.png" alt="image-20230312115542257"></p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20230312115751685.png" alt="image-20230312115751685"></p><p>OSI优点：每层功能都有具体描述</p><p>TCP/IP优点：层次简单，功能交叉少</p><p><img src="C:\Users\HYY\AppData\Roaming\Typora\typora-user-images\image-20230312120010728.png" alt="image-20230312120010728"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>弗洛伊德算法</title>
      <link href="/2022/06/14/fu-luo-yi-de-suan-fa/"/>
      <url>/2022/06/14/fu-luo-yi-de-suan-fa/</url>
      
        <content type="html"><![CDATA[<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> k=<span class="number">0</span>;k&lt;n;k++){<span class="comment">//确定中转点 </span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++){<span class="comment">//遍历整个矩阵，i为行号，j为列号 </span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++){</span><br><span class="line"><span class="keyword">if</span>(A[i][j]&gt;A[i][k]+A[k][j]){<span class="comment">//以k为中转点的路径更短 </span></span><br><span class="line">A[i][j]=A[i][k]+A[k][j];<span class="comment">//更新最短路径 </span></span><br><span class="line">path[i][j]=k;<span class="comment">//更新中转点 </span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>排序算法总结</title>
      <link href="/2022/06/14/shu-ju-jie-gou-pai-xu-suan-fa-zong-jie/"/>
      <url>/2022/06/14/shu-ju-jie-gou-pai-xu-suan-fa-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="数据结构-排序算法总结"><a href="#数据结构-排序算法总结" class="headerlink" title="数据结构-排序算法总结"></a>数据结构-排序算法总结</h1><h2 id="1-插入排序"><a href="#1-插入排序" class="headerlink" title="1.插入排序"></a>1.插入排序</h2><p>插入排序的基本思想是每次将一个待排序的记录按关键字大小插入到前面已排好序的子序列中，直到记录全部插入完成。</p><h3 id="1-1直接插入排序"><a href="#1-1直接插入排序" class="headerlink" title="1.1直接插入排序"></a>1.1直接插入排序</h3><p> 从插入排序的定义我们可以想到一种最为简单的插入排序算法，即确定待排序元素L[i]将要插入的位置k，将待排序序列L[]从k开始一直到L[i-1]的元素全部后移一位，再将L[i]插入k。</p><p>逻辑代码，如下所示：</p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">insertsort</span><span class="params">(<span class="type">int</span> A[],<span class="type">int</span> n)</span>{</span><br><span class="line"><span class="type">int</span> i,j;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;i++){ <span class="comment">//从A[2]开始依次与前面的元素进行比较</span></span><br><span class="line"><span class="keyword">if</span>(A[i]&lt;A[i<span class="number">-1</span>]){ <span class="comment">//如果比前面的元素小</span></span><br><span class="line">A[<span class="number">0</span>]=A[i];<span class="comment">//将当前位置i的元素复制到哨兵处</span></span><br><span class="line"><span class="keyword">for</span>(j=i<span class="number">-1</span>;A[<span class="number">0</span>]&lt;A[j];--j){<span class="comment">//再从i-1开始从后向前进行比较，直到遇到比A[0]小的元素就跳出循环</span></span><br><span class="line">A[j+<span class="number">1</span>]=A[j];<span class="comment">//发现比A[0]大的元素就将其向后移动一位</span></span><br><span class="line">}</span><br><span class="line">A[j+<span class="number">1</span>]=A[<span class="number">0</span>];<span class="comment">//此时比较完成，将待排序元素插入应插入的位置，此处循环中采用--j，因此排序结束后，指针停留的位置为待插入位置的前一个元素处，因此是j+1</span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>空间复杂度为O(1)</p><p>时间复杂度最好的情况下为O(n)，最坏的情况下为O(n^2)</p><p>稳定性分析：稳定，由于直接插入排序每次都是先比较再移动，因此不会出现相同元素相同位置发生变化的情况。</p><p>适用于顺序和链式存储的线性表</p><h3 id="1-2折半插入排序"><a href="#1-2折半插入排序" class="headerlink" title="1.2折半插入排序"></a>1.2折半插入排序</h3><p>在直接插入排序中每次都是先比较再移动，折半插入排序的基本思想是先折半查找出待插入的位置，再统一的移动之后的所有元素。折半插入排序仅减少了比较元素的次数，复杂度为O(nlog2n)，但移动元素的次数并未改变，因此复杂度仍为O(n^2)</p><h3 id="1-3希尔排序"><a href="#1-3希尔排序" class="headerlink" title="1.3希尔排序"></a>1.3希尔排序</h3><p>根据前面的介绍可以看出，直接插入排序对于基本有序的待排序序列有较低的时间复杂度，因此希尔排序采用：将待排序表分割成若干个小的待排序表，实现方式为将相隔某个增量的记录组成一个子表，然后对各个子表进行直接插入排序，直到所选增量减少为1时，对整个待排序表进行一次直接插入排序。</p><p>空间复杂度为O(1)</p><p>时间复杂度最好的情况下为O(n^1.3)，最坏的情况下为O(n^2)</p><p>稳定性分析：不稳定，当相同元素分到不同子表时，相对位置可能会发生改变</p><p>只适用于顺序存储的线性表</p><h2 id="2-交换排序"><a href="#2-交换排序" class="headerlink" title="2.交换排序"></a>2.交换排序</h2><h3 id="2-1冒泡排序"><a href="#2-1冒泡排序" class="headerlink" title="2.1冒泡排序"></a>2.1冒泡排序</h3><p> 基本思想：从后向前（或者从前向后）两两比较相邻的元素，如果前面的元素大于后面的元素就交换次序，直到所有元素都遍历完，这就是第一趟冒泡排序，可以确定最小元素在序列中的位置。每一趟冒泡排序都会有一个元素确定最终位置，就像🐟吐泡泡一样，最小的元素会逐步冒泡到水面上，至多n-1躺排序就可以确定排序序列。</p><p>冒泡排序会受序列初始状态影响，在代码中设立flag变量，一旦某一趟flag没有改变，就说明已经排好序了。</p><p>空间复杂度为O(1)</p><p>时间复杂度最好的情况下为O(n)，最坏的情况下为O(n^2)</p><p>稳定性分析：稳定</p><h3 id="2-2快速排序"><a href="#2-2快速排序" class="headerlink" title="2.2快速排序"></a>2.2快速排序</h3><p>基本思想：分治法。首先在待排序序列中任选一个元素作为基准，然后对待排序序列进行比较，保证基准的左侧都是比基准小的元素，右侧都是比基准大的元素。这样一来就确定了基准的最终位置。之后分别递归的对左右序列重复以上步骤，直到每个序列只有一个元素或者为空就停止排序。</p><p>考研中一般选取第一个元素作为基准。</p><p>空间复杂度：由于快速排序是递归调用的，需要一个递归工作站来保存每层调用的递归信息，因此其容量与递归调用深度一致，最好的情况下为O(log2n)，最坏情况下为需要进行n-1躺递归，为O(n)。平均为O(log2n)</p><p>时间复杂度：快速排序算法的时间复杂度与子表的划为是否均衡有关，最坏情况下左子表为n-1，右子表为0，每次需要n-1躺递归，时间复杂度为O(n^2)，最好的情况下左右子表都不超过n/2，时间复杂度为O(nlog2n)，快速排序算法是所有内部排序算法中平均性能最优的算法。</p><p>稳定性分析：不稳定，当选取相同元素作为基准时，元素的相对位置会发生改变。</p><p>*当快速排序的序列基本有序时，不利于发挥快速排序的优势，因为这会导致快速排序划分匹配区间不均匀，每次选取第n个关键字进行划分。</p><h2 id="3-选择排序"><a href="#3-选择排序" class="headerlink" title="3.选择排序"></a>3.选择排序</h2><p>基本思想，第i躺选择一个最小元素插入有序序列L[i]中。经过n-1躺排序完成。</p><h3 id="3-1简单选择排序"><a href="#3-1简单选择排序" class="headerlink" title="3.1简单选择排序"></a>3.1简单选择排序</h3><p>基本思想，第i躺从i-n中选择一个最小元素与L[i]<strong>交换</strong>。经过n-1躺排序完成。</p><p>空间复杂度为O(1)</p><p>时间复杂度为O(n^2)，简单选择排序中元素移动的次数较少，一般不超过3(n-1)次，最好的情况是移动0次，即有序。但元素间的比较次数与初始状态无关，始终是n(n-1)/2次。</p><p>稳定性分析：不稳定</p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">selectqort</span><span class="params">(<span class="type">int</span> A[],<span class="type">int</span> n)</span>{</span><br><span class="line">    <span class="type">int</span> i,j,min,t;</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++){</span><br><span class="line">        min=i;</span><br><span class="line">        <span class="keyword">for</span>(j=i+<span class="number">1</span>;j&lt;n;j++){</span><br><span class="line">            <span class="keyword">if</span>(A[j]&lt;A[min){</span><br><span class="line">                min=j;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span>(min!=i){</span><br><span class="line">            t=A[i];</span><br><span class="line">            A[i]=A[min];</span><br><span class="line">            A[min]=t;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="3-2堆排序"><a href="#3-2堆排序" class="headerlink" title="3.2堆排序"></a>3.2堆排序</h3><p>堆：n个关键字序列成为堆。可以将其视为完全二叉树，根节点大于左右孩子的成为大根堆，反之称为小根堆。</p><p>基本思想：首先将待排序序列建成初始堆(大，小)，以大根堆为例，堆顶元素就是最大值，将堆顶元素输出后，把最后一个堆底元素放置到堆顶，此时堆不符合大根堆特性，重新建立大根堆。重复以上步骤，直到堆只剩一个元素为止。</p><p>堆排序的关键是构造初始堆。</p><p>空间复杂度为O(1)</p><p>时间复杂度：建堆时间为O(n)，之后有n-1次向下调整操作，每次O(h)的时间复杂度，时间复杂度为O(nlog2n)。</p><p>稳定性分析：不稳定</p><h2 id="4-归并排序和基数排序"><a href="#4-归并排序和基数排序" class="headerlink" title="4.归并排序和基数排序"></a>4.归并排序和基数排序</h2><h3 id="4-1归并排序"><a href="#4-1归并排序" class="headerlink" title="4.1归并排序"></a>4.1归并排序</h3><p>基本思想：将n个有序序列归并成一个有序序列。一个含有n个关键字的序列可以看成含有n个有序序列，将这些序列两两归并就可以得到n/2个含有2或者1个关键字的有序序列。重复以上步骤，直到合并成一个长度为n的有序序列，归并完成。这种方法成为2路归并。相应的有多路归并。</p><p>空间复杂度为O(n),代码实现中需要将总长度为n的子表复制到另一个一维数组中去，因此需要n的辅助空间。</p><p>时间复杂度：每趟归并需要O(n)的时间复杂度，总共需要进行long2n躺，因此时间复杂度O(nlog2n)。</p><p>稳定性分析：稳定</p><h3 id="4-2基数排序"><a href="#4-2基数排序" class="headerlink" title="4.2基数排序"></a>4.2基数排序</h3><p>基数排序是一种很特别的排序方法，不基于比较和移动，而是基于关键字的大小进行排序。 一般用于日期等特殊类型，对日，月，年等多关键字进行排序，实现对日期这种单逻辑关键字的排序。</p><p>实现多关键字排序一般有两种思想，分别是最高位优先(MSD)和最低位优先(LSD)。简单来讲就是按照关键字的全总递增或递减划为若干子序列，最后再组合成一个有序序列。</p><p>基数排序中，每趟排序都根据相应的关键字进行一趟分配和收集。</p><p>例如对一串具有三位数的序列进行排序，第一趟根据个位的大小分配到相应的队列，然后将得到的各个子序列组合成新的序列。第二趟根据十位的大小进行排序，然后将得到的各个子序列组合成新的序列。第三躺根据百位的大小进行排序，然后将得到的各个子序列组合成最终的序列。</p><p>空间复杂度为O(r),代码实现中需要r个辅助队列，r为关键字能取到的所有值。</p><p>时间复杂度：基数排序需要进行d躺分配和收集，每次分配需要O(n)，每次收集需要O(r)，所以总的时间复杂度为O(d(n+r))。</p><p>稳定性：稳定。</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>广度优先搜索</title>
      <link href="/2022/06/10/guang-du-you-xian-sou-suo-suan-fa/"/>
      <url>/2022/06/10/guang-du-you-xian-sou-suo-suan-fa/</url>
      
        <content type="html"><![CDATA[<p>​      广度优先搜索算法类似二叉树的层序遍历算法，首先访问指定节点v,再从v出发依次访问与v的邻接节点m…..等，直到将图中节点遍历完成。</p><p>​      在无向图中，邻接节点的邻接节点包含已访问过的节点，重复的访问会增加BFS算法的时间复杂度，因此在BFS算法中，实际每次访问的都是未被访问的节点，在代码实现中通常建立一个visit数组，用于存储各节点的访问状态，只寻找访问状态为false的节点。</p><p>​       对于使用邻接表进行存储的图，同一个图，由于邻接表中节点的存储顺序不同，广度优先遍历序列并不唯一。</p><p>​       由于广度优先算法不存在回退这种状态，因此需要建立一个辅助队列以存储当前顶点的下一层节点。</p><p>​       c语言代码实现如下：</p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MVNum 100 <span class="comment">//最大顶点数 </span></span></span><br><span class="line"><span class="comment">//创建一个中断函数 </span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Interrupt</span><span class="params">(<span class="type">void</span>)</span><span class="comment">//创建一个中断函数 </span></span><br><span class="line">{</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)<span class="comment">//用于检测换行符，使函数脱离scanf的连续输出 </span></span><br><span class="line"><span class="keyword">if</span>(getchar()==<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">} </span><br><span class="line"><span class="comment">//引入队列，这里是顺序队列 </span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line"><span class="type">int</span> data[MVNum];<span class="comment">//分配给队列一个数组空间 </span></span><br><span class="line"><span class="type">int</span> front;<span class="comment">//队列头 </span></span><br><span class="line"><span class="type">int</span> rear;<span class="comment">//队列尾 </span></span><br><span class="line">}SqQueue;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">InitQueue</span><span class="params">(SqQueue &amp;Q)</span><span class="comment">//初始化队列 </span></span><br><span class="line">{</span><br><span class="line">Q.front = Q.rear = <span class="number">0</span>;<span class="comment">//使队列头和队列尾都为0 </span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">EnQueue</span><span class="params">(SqQueue &amp;Q,<span class="type">int</span> e)</span><span class="comment">//入队 ，由于是循环队列，故少用一个元素空间，该函数在有MAXSIZE-1个元素时便已判断为满 </span></span><br><span class="line">{</span><br><span class="line"><span class="keyword">if</span>((Q.rear+<span class="number">1</span>)%MVNum == Q.front)<span class="comment">//判断队列是否为满 ，这里是循环队列队列满的条件是 (Q.rear+1)%MAXSIZE == Q.front</span></span><br><span class="line">{</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"队列已满！\n"</span>);</span><br><span class="line">}</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">{</span><br><span class="line"><span class="keyword">if</span>(Q.rear == MVNum)<span class="comment">//如果队尾超出最大值但队列又不满，便使其对最大值求余运算 </span></span><br><span class="line">Q.rear = Q.rear%MVNum;</span><br><span class="line">Q.data[Q.rear] = e;<span class="comment">//使变量e的值入队 </span></span><br><span class="line">Q.rear++;<span class="comment">//并使队尾加一 </span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> <span class="title function_">QueueEmpty</span><span class="params">(SqQueue Q)</span><span class="comment">//队列判空操作 </span></span><br><span class="line">{</span><br><span class="line"><span class="keyword">if</span>(Q.front == Q.rear)<span class="comment">//如果队列为空，返回true，否则返回false </span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">DeQueue</span><span class="params">(SqQueue &amp;Q)</span><span class="comment">//出队 </span></span><br><span class="line">{</span><br><span class="line"><span class="type">int</span> a = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">if</span>(QueueEmpty(Q))<span class="comment">//首先判断队列是否为空，队列为空的条件是 Q.front == Q.rear</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"队列为空！\n"</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">{</span><br><span class="line">a = Q.data[Q.front];<span class="comment">//导出队头元素数据 </span></span><br><span class="line">Q.front++;<span class="comment">//使队头加一 </span></span><br><span class="line"><span class="keyword">if</span>(!QueueEmpty(Q))<span class="comment">//在队列非空的情况下，如果队头等于最大值，也对最大值做求余运算 </span></span><br><span class="line">Q.front = Q.front%MVNum;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> a;</span><br><span class="line">}</span><br><span class="line"><span class="comment">//创建矩阵表操作 </span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">ArcNode</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line"><span class="type">int</span> adjvex;<span class="comment">//该边所指向的顶点的位置 </span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ArcNode</span> *<span class="title">nextarc</span>;</span><span class="comment">//该向下一条边的指针 </span></span><br><span class="line"><span class="type">int</span> weight;<span class="comment">//权值 </span></span><br><span class="line">}ArcNode;</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">VNode</span>//顶点信息 </span></span><br><span class="line"><span class="class">{</span></span><br><span class="line"><span class="type">char</span> data;<span class="comment">//顶点名称 </span></span><br><span class="line">ArcNode *firstarc;<span class="comment">//指向第一条依附该顶点的边的指针 </span></span><br><span class="line">}VNode,AdjList[MVNum];<span class="comment">//AdjList表示邻接表类型 </span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>//邻接表 </span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">AdjList vertices;</span><br><span class="line"><span class="type">int</span> vexnum,arcnum;<span class="comment">//图的当前顶点数和边数 </span></span><br><span class="line">}ALGraph;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">InitGraph</span><span class="params">(ALGraph &amp;G)</span><span class="comment">//图的初始化 </span></span><br><span class="line">{</span><br><span class="line"><span class="type">int</span> i;</span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;MVNum;i++)</span><br><span class="line">G.vertices[i].firstarc = <span class="literal">NULL</span>;<span class="comment">//使所有的第一个结点都置空，也就是后面设定的尾指针的判空操作 </span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">CreateGraph</span><span class="params">(ALGraph &amp;G)</span><span class="comment">//图的创建 </span></span><br><span class="line">{</span><br><span class="line"><span class="type">int</span> i;<span class="comment">//记录次数 </span></span><br><span class="line"><span class="type">char</span> a;<span class="comment">//顶点变量 </span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"请输入顶点数和边数:"</span>);</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%d %d"</span>,&amp;G.vexnum,&amp;G.arcnum);<span class="comment">//顶点数和边数的赋值 </span></span><br><span class="line">Interrupt();<span class="comment">//该函数用于检测并吸收换行符 </span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"请输入顶点名称（连续输入）："</span>);</span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;G.vexnum;i++)<span class="comment">//利用循环输入图中顶点名称 </span></span><br><span class="line">{</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%c"</span>,&amp;a);</span><br><span class="line">G.vertices[i].data = a;<span class="comment">//第i个顶点的命名 </span></span><br><span class="line">}</span><br><span class="line">Interrupt();<span class="comment">//该函数用于检测并吸收换行符</span></span><br><span class="line"><span class="type">char</span> b,c;<span class="comment">//顶点变量 </span></span><br><span class="line"><span class="type">int</span> w,j,k;<span class="comment">//w为权值变量，j和k是用来记录次数的 </span></span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;G.arcnum;i++)<span class="comment">//利用循环输入所有边的两个顶点和权值 </span></span><br><span class="line">{</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"请输入边的两个顶点:"</span>);</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%c %c"</span>,&amp;b,&amp;c);<span class="comment">//输入 </span></span><br><span class="line">Interrupt();<span class="comment">//该函数用于检测并吸收换行符</span></span><br><span class="line"><span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;G.arcnum;j++)<span class="comment">//该操作为书上的函数LocateVex操作 </span></span><br><span class="line">{</span><br><span class="line"><span class="keyword">if</span>(G.vertices[j].data == b)<span class="comment">//找到输入的顶点b的位置 </span></span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">for</span>(k=<span class="number">0</span>;k&lt;G.arcnum;k++)</span><br><span class="line">{</span><br><span class="line"><span class="keyword">if</span>(G.vertices[k].data == c)<span class="comment">//找到输入的顶点c的位置 </span></span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">}</span><br><span class="line">ArcNode *p1,*p2;<span class="comment">//创建两个野结点 </span></span><br><span class="line">p1 = (ArcNode*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(ArcNode));</span><br><span class="line">p1-&gt;adjvex = k;</span><br><span class="line">p1-&gt;weight = <span class="number">1</span>;<span class="comment">//权值赋值 </span></span><br><span class="line">p1-&gt;nextarc = G.vertices[j].firstarc;<span class="comment">//类似于头插法 </span></span><br><span class="line">G.vertices[j].firstarc = p1;<span class="comment">//并使头结点永远放在第一位 </span></span><br><span class="line">p2 = (ArcNode*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(ArcNode));</span><br><span class="line">p2-&gt;adjvex = j;</span><br><span class="line">p2-&gt;weight = <span class="number">1</span>;</span><br><span class="line">p2-&gt;nextarc = G.vertices[k].firstarc;</span><br><span class="line">G.vertices[k].firstarc = p2;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">InputGraph</span><span class="params">(ALGraph G)</span><span class="comment">//邻接表的输出 </span></span><br><span class="line">{</span><br><span class="line"><span class="type">int</span> i,j;<span class="comment">//记录次数 </span></span><br><span class="line">ArcNode *p1;<span class="comment">//用于遍历链表 </span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"邻接表为：\n"</span>);</span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;G.vexnum;i++)<span class="comment">//利用循环输出 </span></span><br><span class="line">{</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%c"</span>,G.vertices[i].data);</span><br><span class="line">p1 = G.vertices[i].firstarc;</span><br><span class="line"><span class="keyword">while</span>(p1)<span class="comment">//当p为空时，结束循环 </span></span><br><span class="line">{</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">" --&gt; %d"</span>,p1-&gt;adjvex);</span><br><span class="line">p1 = p1-&gt;nextarc;<span class="comment">//p指向p的下一个结点 </span></span><br><span class="line">}</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="comment">//广度优先搜索遍历 </span></span><br><span class="line">ArcNode *p;<span class="comment">//创建一个全局变量，以便于进行查找 </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">FirstAdjVex</span><span class="params">(ALGraph G,<span class="type">int</span> u)</span><span class="comment">//表示u的第一个邻接点 </span></span><br><span class="line">{</span><br><span class="line">p = G.vertices[u].firstarc;<span class="comment">//全局变量的赋值 </span></span><br><span class="line"><span class="keyword">if</span>(p == <span class="literal">NULL</span>)<span class="comment">//如果头结点的下一个结点为空，返回负数，否则返回p结点的值 </span></span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">return</span> p-&gt;adjvex;</span><br><span class="line">}</span><br><span class="line"><span class="type">int</span> <span class="title function_">NextAdjVex</span><span class="params">(ALGraph G)</span><span class="comment">//下一个邻接点 </span></span><br><span class="line">{</span><br><span class="line">p = p-&gt;nextarc;<span class="comment">//由于p为全局变量，这里直接指向下一个便是 </span></span><br><span class="line"><span class="keyword">if</span>(p == <span class="literal">NULL</span>)<span class="comment">//如果头结点的下一个结点为空，返回负数，否则返回p结点的值  </span></span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">return</span> p-&gt;adjvex;</span><br><span class="line">}</span><br><span class="line"><span class="type">bool</span> visited[MVNum];<span class="comment">//访问标志数组 ,其初始值为false </span></span><br><span class="line"><span class="type">void</span> <span class="title function_">InitVisited</span><span class="params">(<span class="type">bool</span> *visited)</span><span class="comment">//标志数组初始化 </span></span><br><span class="line">{</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;MVNum;i++)</span><br><span class="line">visited[i] = <span class="literal">false</span>;</span><br><span class="line">} </span><br><span class="line"><span class="type">void</span> <span class="title function_">BFS</span><span class="params">(ALGraph G,<span class="type">int</span> v)</span><span class="comment">//广度优先搜索遍历 ，非递归形式 </span></span><br><span class="line">{</span><br><span class="line"><span class="type">int</span> u;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%c"</span>,G.vertices[v].data);</span><br><span class="line">visited[v] = <span class="literal">true</span>;<span class="comment">//访问第v个顶点，并置访问标志数组相应分量值为true </span></span><br><span class="line">SqQueue Q;</span><br><span class="line">InitQueue(Q);<span class="comment">//引入队列，初始化 </span></span><br><span class="line">EnQueue(Q,v);<span class="comment">//入队 </span></span><br><span class="line"><span class="keyword">while</span>(!QueueEmpty(Q))<span class="comment">//队列非空 </span></span><br><span class="line">{</span><br><span class="line">u = DeQueue(Q);<span class="comment">//出队，别赋值给变量u </span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> w=FirstAdjVex(G,u);w&gt;=<span class="number">0</span>;w=NextAdjVex(G))<span class="comment">//依次检查u的所有邻接点w,w&gt;=0表示存邻接点</span></span><br><span class="line"><span class="comment">//函数FirstAdjVex(G,u)表示u的第一个邻接点，函数NextAdjVex(G)表示相对于w的下一个邻接点 </span></span><br><span class="line"><span class="keyword">if</span>(!visited[w])<span class="comment">//w为u的尚未访问的邻接点 </span></span><br><span class="line">{</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%c"</span>,G.vertices[w].data);</span><br><span class="line">visited[w] = <span class="literal">true</span>;<span class="comment">//访问第w个顶点，并置访问标志数组相应分量值为true </span></span><br><span class="line">EnQueue(Q,w);<span class="comment">//w入队 </span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">{</span><br><span class="line">ALGraph G;</span><br><span class="line">InitGraph(G);<span class="comment">//初始化 </span></span><br><span class="line">CreateGraph(G);<span class="comment">//邻接表的创建 </span></span><br><span class="line">InputGraph(G);<span class="comment">//邻接表的输出 </span></span><br><span class="line">BFS(G,<span class="number">2</span>);<span class="comment">//广度优先搜索遍历 ，其中的2为测试值，可更换成变量 </span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>NAT转换</title>
      <link href="/2022/06/08/nat-di-zhi-zhuan-huan/"/>
      <url>/2022/06/08/nat-di-zhi-zhuan-huan/</url>
      
        <content type="html"><![CDATA[<p>﻿<strong>NAT地址转换</strong><br>现有的网络体系中，我们大多使用ipv4地址进行上网操作，ipv4最大地址为2^32<br>为了便于对IP地址进行管理， 根据IPv4地址的第一个字节，IPv4地址可以分为以下五类。</p><p>A类：0~127</p><p>B类：128~191</p><p>C类：192~223</p><p>D类：224~239，组播地址</p><p>E类：240~254，保留为研究测试使用</p><p>其中私有地址空间为<br><img src="https://img-blog.csdnimg.cn/20210129113120398.png" alt="在这里插入图片描述">私有地址无法直接访问公网</p><p>为了节省紧缺的ipv4地址空间，我们通常采用NAT    地址转换技术<br>NAT分为以下三类：<br><img src="https://img-blog.csdnimg.cn/20210129113301349.png" alt="在这里插入图片描述"><br>以下是NAT转换中涉及的名词<br><img src="https://img-blog.csdnimg.cn/20210129113517270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDk5MDIzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>NAT会在路由器中生成一张NAT表，在转换时路由器就是通过这张表来转换私有地址的。</p><p>1.静态NAT<br>静态NAT就是在内网用户访问外网时，将内网用户的私有地址转换为公网地址，一个内网用户就要对应一个公网地址，这对于节省ipv4地址意义不大，所以我们经常将静态NAT用在内网中供外网访问的服务器上。<br>具体配置流程如下：<br><img src="https://img-blog.csdnimg.cn/20210129114341199.png" alt="在这里插入图片描述"><br>2.动态NAT<br>动态NAT与静态NAT一样，一个内网用户对应一个公网地址，但不同的是，动态NAT中，我们采用NAT地址池，用户在访问外网时会用地址池动态的获取公网地址。<br>具体配置流程如下：<br><img src="https://img-blog.csdnimg.cn/20210129114616866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDk5MDIzOQ==,size_16,color_FFFFFF,t_70" alt=""><br>值得注意的是在上述配置流程中我们采用了ACL来指定内网中可以访问外网的用户地址段。这就是ACL 的第二个作用。</p><p>3.端口复用<br>端口复用我们也称他为PAT，即端口地址转换<br><img src="https://img-blog.csdnimg.cn/20210129115154225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDk5MDIzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>具体配置流程如下：<br><img src="https://img-blog.csdnimg.cn/20210129115239766.png" alt="在这里插入图片描述"></p><p>在配置PAT时，首先我们要配置允许访问的ACL。然后再将它应用到NAT路由器的outside接口上，接口采用overload过载模式。这样数据包在通过这个接口时，就会在ip地址后加上端口号。一个接口可允许4000个端口号。端口号的知识大家可以自行去查询，我们在浏览器中访问一个网站就是访问它HTTP服务器的80端口。<br>最后查询NAT转换配置的命令是<br>show ip nat translations<br>当然在有数据包通过的时候才能查询到相关内容</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>杨辉三角形</title>
      <link href="/2022/06/08/acm-dao-li-yang-hui-san-jiao-xing/"/>
      <url>/2022/06/08/acm-dao-li-yang-hui-san-jiao-xing/</url>
      
        <content type="html"><![CDATA[<p>﻿Fans喜欢图形，而且喜欢把图形倒过来欣赏。有一次，他看见杨辉三角形 了，觉得很新鲜，于是就把它们大大小小地摆布出来。输入一些整数n（1≤n≤10），读入其每个整数，以该整数为行数，其画出来的倒杨辉三角形（每相邻2个数据由一个空格隔开）就是fans所喜欢欣赏的。Fans是手工做的，你却可以用编程更快捷地做出来，多爽啊!<br>样例输入</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">5</span><br><span class="line">3</span><br></pre></td></tr></tbody></table></figure><p>样例输出</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1 4 6 4 1</span><br><span class="line"> 1 3 3 1</span><br><span class="line">  1 2 1</span><br><span class="line">   1 1</span><br><span class="line">    1</span><br><span class="line">1 2 1</span><br><span class="line"> 1 1</span><br><span class="line">  1</span><br></pre></td></tr></tbody></table></figure><p>解题思路：<br>首先将杨辉三角形存储到二维数组；<br>1<br>11<br>121<br>1331<br>然后倒过来输出，在输出的同时注意题目中要求 每相邻2个数据由一个空格隔开。</p><p>代码：</p><figure class="highlight javascript"><table><tbody><tr><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int <span class="title function_">main</span>(<span class="params"></span>)</span><br><span class="line">{</span><br><span class="line">int n;</span><br><span class="line"><span class="keyword">while</span>(cin&gt;&gt;n)</span><br><span class="line">{</span><br><span class="line">int a[n][n]={};</span><br><span class="line">int i,j,k;</span><br><span class="line"><span class="keyword">for</span>( i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">{</span><br><span class="line"><span class="keyword">for</span>( j=<span class="number">0</span>;j&lt;=i;j++)</span><br><span class="line">{</span><br><span class="line"><span class="keyword">if</span>(i==j||j==<span class="number">0</span>)</span><br><span class="line">{</span><br><span class="line">a[i][j]=<span class="number">1</span>;</span><br><span class="line">} </span><br><span class="line"> <span class="keyword">else</span></span><br><span class="line"> {</span><br><span class="line"> a[i][j]=a[i-<span class="number">1</span>][j]+a[i-<span class="number">1</span>][j-<span class="number">1</span>];<span class="comment">//杨辉三角形中间的元素是左上角的元素与右上角的元素和</span></span><br><span class="line"> }</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">{</span><br><span class="line"><span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;i;j++)</span><br><span class="line">cout&lt;&lt;<span class="string">" "</span>;<span class="comment">//输出空格</span></span><br><span class="line"><span class="keyword">for</span>(k=<span class="number">0</span>;k&lt;n-i;k++)</span><br><span class="line">{</span><br><span class="line">cout&lt;&lt;a[n-i-<span class="number">1</span>][k];<span class="comment">//从最后一行开始输出，即i=0；</span></span><br><span class="line">cout&lt;&lt;<span class="string">" "</span>;</span><br><span class="line">}</span><br><span class="line">cout&lt;&lt;endl;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>KMP</title>
      <link href="/2022/06/06/shu-ju-jie-gou-kmp-suan-fa-c-yu-yan-shi-xian/"/>
      <url>/2022/06/06/shu-ju-jie-gou-kmp-suan-fa-c-yu-yan-shi-xian/</url>
      
        <content type="html"><![CDATA[<p>﻿最近学习数据结构串的匹配，王道书上只有伪代码实现，于是尝试写出可以运行的c语言代码。</p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">Next</span><span class="params">(<span class="type">char</span>* T,<span class="type">int</span>* next)</span>{<span class="comment">//求next数组</span></span><br><span class="line">next[<span class="number">1</span>]=<span class="number">0</span>;</span><br><span class="line">next[<span class="number">2</span>]=<span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> j=<span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> i=<span class="number">2</span>;</span><br><span class="line"><span class="keyword">while</span>(i&lt;<span class="built_in">strlen</span>(T)){</span><br><span class="line"><span class="keyword">if</span>(j==<span class="number">0</span>||T[j<span class="number">-1</span>]==T[i<span class="number">-1</span>]){</span><br><span class="line">i++;</span><br><span class="line">j++;</span><br><span class="line"><span class="keyword">if</span>(T[j<span class="number">-1</span>]!=T[i<span class="number">-1</span>]){</span><br><span class="line">next[i]=j;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">else</span>{</span><br><span class="line">next[i]=next[j];</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="keyword">else</span>{</span><br><span class="line">j=next[j];</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">KMP</span><span class="params">(<span class="type">char</span>* S,<span class="type">char</span>* T)</span>{<span class="comment">//KMP算法实现</span></span><br><span class="line">  <span class="type">int</span> next[<span class="number">10</span>];</span><br><span class="line">  Next(T,next);</span><br><span class="line">  <span class="type">int</span> i=<span class="number">1</span>;</span><br><span class="line">  <span class="type">int</span> j=<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">while</span>(i&lt;=<span class="built_in">strlen</span>(S)&amp;&amp;j&lt;=<span class="built_in">strlen</span>(T)){</span><br><span class="line">    <span class="keyword">if</span>(j==<span class="number">0</span>||S[i<span class="number">-1</span>]==T[j<span class="number">-1</span>]){</span><br><span class="line">  i++;</span><br><span class="line">  j++;</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">else</span>{</span><br><span class="line">  j=next[j];</span><br><span class="line">  }</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span>(j&gt;<span class="built_in">strlen</span>(T)){</span><br><span class="line">  <span class="keyword">return</span> i-(<span class="type">int</span>)<span class="built_in">strlen</span>(T);</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>{</span><br><span class="line"><span class="type">char</span>* source;</span><br><span class="line">source= (<span class="type">char</span>*)<span class="built_in">malloc</span>(<span class="number">100</span>*<span class="keyword">sizeof</span>(<span class="type">char</span>));</span><br><span class="line">    <span class="type">char</span>* target;</span><br><span class="line">target= (<span class="type">char</span>*)<span class="built_in">malloc</span>(<span class="number">100</span>*<span class="keyword">sizeof</span>(<span class="type">char</span>));</span><br><span class="line">gets(source);</span><br><span class="line">gets(target);</span><br><span class="line">    <span class="type">int</span> i=KMP(source,target);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d"</span>,i);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KMP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算子串出现次数</title>
      <link href="/2022/06/06/wen-ti-a-string-ji-suan-zi-chuan-chu-xian-ci-shu/"/>
      <url>/2022/06/06/wen-ti-a-string-ji-suan-zi-chuan-chu-xian-ci-shu/</url>
      
        <content type="html"><![CDATA[<p>﻿</p><p>题目描述<br>给定完全由小写字母组成的两个字符串a和b，求b在a中出现的次数</p><p>输入<br>第一行为字符串a；</p><p>第二行为字符串b;</p><p>输出<br>一个整数，表示b在a中出现的次数</p><p>样例输入</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">sharonisagoodgirlihopeshecangetgoodgradesinthefinalexams</span><br><span class="line">in</span><br></pre></td></tr></tbody></table></figure><p>样例输出</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">2</span><br></pre></td></tr></tbody></table></figure><p>c++中，我常用substr(n,m)函数(从n开始截取长为m的部分字符串)来截取字符串长度，来比对截取部分的代码与子串是否相同。<br>代码：<br></p><figure class="highlight javascript"><table><tbody><tr><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;string&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int <span class="title function_">main</span>(<span class="params"></span>)</span><br><span class="line">{</span><br><span class="line">string a;</span><br><span class="line">string b;</span><br><span class="line">int count=<span class="number">0</span>;</span><br><span class="line">cin&gt;&gt;a;</span><br><span class="line">cin&gt;&gt;b;</span><br><span class="line"><span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;a.<span class="title function_">length</span>();i++)</span><br><span class="line">{</span><br><span class="line"><span class="keyword">if</span>(a.<span class="title function_">substr</span>(i,b.<span class="title function_">length</span>())==b)</span><br><span class="line">{</span><br><span class="line">count++;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">cout&lt;&lt;count&lt;&lt;endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">} </span><br></pre></td></tr></tbody></table></figure><p></p>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> string </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
